{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "# For votingclassifier ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Support vector machine classifier - details in https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "from sklearn.svm import SVC\n",
    "# StandardScaler for scaling the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensemble Learning\n",
    "----------------------\n",
    "## a. VotingClassifier\n",
    "----------------------\n",
    "### VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\n",
    "* #### Arguments: only \"estimators\" matters for now\n",
    " 1.         estimators: list of (str, estimator) tuples - Invoking the fit method on the VotingClassifier will fit clones of those original estimators that will be stored in the class attribute self.estimators_. An estimator can be set to 'drop' using set_params.\n",
    "  2.         voting : {‘hard’, ‘soft’}, default=’hard’- If ‘hard’, uses predicted class labels for majority rule voting. Else if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.\n",
    "  3.       weights : array-like of shape (n_classifiers,), default=None - Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) or class probabilities before averaging (soft voting). Uses uniform weights if None.\n",
    "  4.         n_jobs : int, default=None - The number of jobs to run in parallel for fit. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. \n",
    "  5.         flatten_transform : bool, default=True - Affects shape of transform output only when voting=’soft’ If voting=’soft’ and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes). If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes).\n",
    "  6.         verbose : bool, default=False - If True, the time elapsed while fitting will be printed as it is completed.\n",
    "* #### Artributes: (of the VotingClassifier object)\n",
    " *         estimators_ : list of classifiers - The collection of fitted sub-estimators as defined in estimators that are not ‘drop’.\n",
    " *         classes_ : array-like of shape (n_predictions,) - The classes labels.\n",
    "* #### Methods: (on the VotingClassifier object)\n",
    " *         fit(self, X, y[, sample_weight]) : Fit the estimators.\n",
    " *         fit_transform(self, X[, y]) : Fit to data, then transform it.\n",
    " *         get_params(self[, deep]) : Get the parameters of an estimator from the ensemble.\n",
    " *         predict(self, X) : Predict class labels for X.\n",
    " *         score(self, X, y[, sample_weight]) : Return the mean accuracy on the given test data and labels.\n",
    " *         set_params(self, \\*\\*params) : Set the parameters of an estimator from the ensemble.\n",
    " *         transform(self, X) : Return class labels or probabilities for X for each estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in the dataset\n",
    "df = pd.read_csv('dataset/datasets_diabetes.csv')\n",
    "#take a look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X : dataset feature, y : dataset label\n",
    "X = df.drop(columns = ['Outcome'])\n",
    "y = df['Outcome']\n",
    "# normalize dataset for training\n",
    "X = StandardScaler().fit_transform(X)\n",
    "#split data into train and test sets - Ratio : 4:1 (4 for training, 1 for test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises\n",
    "# EX 3.a.1: Create three models\n",
    "\n",
    "#create new a knn model\n",
    "##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "# Hint: knn is a default KNeighborsClassifier object. \n",
    "knn = None\n",
    "#create a dictionary of all values we want to test for n_neighbors\n",
    "params_knn = {'n_neighbors': np.arange(1, 25)}\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_gs = GridSearchCV(knn, params_knn, cv=5)\n",
    "#fit model to training data\n",
    "knn_gs.fit(X_train, y_train)\n",
    "\n",
    "##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "#create a new logistic regression model\n",
    "# Hint: log_reg is a default LogisticRegression object.\n",
    "log_reg = None\n",
    "#fit the model to the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "# create new support vector machine classifier\n",
    "# Hint: svc is a SVC object which enable probability estimates. \n",
    "# Hint: It can be handled by 'probability' parameter whose value is boolean.\n",
    "# Important : Use probabilistic Support Vector Machine Classifier, \n",
    "# which outputs class probability(For soft ensemble, as we will describe later)\n",
    "svc = None\n",
    "#fit the model to the training data\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Check the accuracy score of all 3 model. for diabete prediction\n",
    "print('knn: {}'.format(knn_gs.score(X_test, y_test)))\n",
    "print('svc: {}'.format(svc.score(X_test, y_test)))\n",
    "print('log_reg: {}'.format(log_reg.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex 3.a.2: Create weighted majority ensemble model\n",
    "\n",
    "##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "# create a 'estimators' for VotingClassifier parameter - Hard voting(Majority rule)\n",
    "# Hint: Parameter 'estimators' of VotingClassifier : list of (str, estimator) tuples\n",
    "estimators=None\n",
    "\n",
    "# create VotingClassifier estimator - Weighted majority ensemble model\n",
    "# Hint: If the parameter 'voting' is `hard', uses predicted class labels for majority rule voting. \n",
    "# Hint: Else if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.\n",
    "ensemble_hard = None\n",
    "# Train weight of VotingClassifier object for ensemble using training dataset\n",
    "ensemble_hard.fit(X_train, y_train)\n",
    "# score the result of ensemble model on test dataset\n",
    "ensemble_hard.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex 3.a.3: Create soft voting model\n",
    "\n",
    "##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "# create a VotingClassifier estimator - Soft voting(Average class probability)\n",
    "# Hint: If the parameter 'voting' is `hard', uses predicted class labels for majority rule voting. \n",
    "# Hint: Else if ‘soft’, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers.\n",
    "ensemble_soft = None\n",
    "ensemble_soft.fit(X_train, y_train)\n",
    "ensemble_soft.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "# obtain class probabilities for all 4 models - KNN, Logistic Regression, SVC, ensemble\n",
    "probas = [c.predict_proba(X_test) for c in (knn_gs, log_reg, svc, ensemble_soft)]\n",
    "# get class probabilities for the first sample in the dataset\n",
    "class1_1 = [pr[0, 0] for pr in probas]\n",
    "class2_1 = [pr[0, 1] for pr in probas]\n",
    "\n",
    "N = 4  # number of groups\n",
    "ind = np.arange(N)  # group positions\n",
    "width = 0.35  # bar width\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# bars for classifier 1-3\n",
    "p1 = ax.bar(ind, np.hstack(([class1_1[:-1], [0]])), width,\n",
    "            color='green', edgecolor='k')\n",
    "p2 = ax.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width,\n",
    "            color='lightgreen', edgecolor='k')\n",
    "\n",
    "# bars for VotingClassifier\n",
    "p3 = ax.bar(ind, [0, 0, 0, class1_1[-1]], width,\n",
    "            color='blue', edgecolor='k')\n",
    "p4 = ax.bar(ind + width, [0, 0, 0, class2_1[-1]], width,\n",
    "            color='steelblue', edgecolor='k')\n",
    "\n",
    "# plot annotations\n",
    "plt.axvline(2.8, color='k', linestyle='dashed')\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(['KNN',\n",
    "                    'Logistic Regression',\n",
    "                    'SVC',\n",
    "                    'VotingClassifier\\n(average probabilities)'],\n",
    "                   rotation=40,\n",
    "                   ha='right')\n",
    "plt.ylim([0, 1.3])\n",
    "plt.title('Class probabilities for sample 1 by different classifiers')\n",
    "plt.legend([p1[0], p2[0]], ['class 1', 'class 2'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensemble Learning\n",
    "----------------------\n",
    "## b. AdaBoostClassifier (Boosting)\n",
    "----------------------\n",
    "* An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases\n",
    "### AdaBoostClassifier(base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
    " #### Arguments: only \"n_estimators\" matters for now\n",
    " 1.         base_estimator : object, default=None - The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes. If None, then the base estimator is DecisionTreeClassifier(max_depth=1).\n",
    "  2.        n_estimators : int, default=50 - The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.\n",
    "  3.        learning_rate : float, default=1 - Learning rate shrinks the contribution of each classifier by learning_rate. There is a trade-off between learning_rate and n_estimators.\n",
    "  4.        algorithm : {‘SAMME’, ‘SAMME.R’}, default=’SAMME.R’- If ‘SAMME.R’ then use the SAMME.R real boosting algorithm. base_estimator must support calculation of class probabilities. If ‘SAMME’ then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations.\n",
    "  5.        random_state : int or RandomState, default=None - Controls the random seed given at each base_estimator at each boosting iteration. Thus, it is only used when base_estimator exposes a random_state. Pass an int for reproducible output across multiple function calls.\n",
    "  6.         verbose : bool, default=False - If True, the time elapsed while fitting will be printed as it is completed.\n",
    "  \n",
    " #### Artributes: (of the VotingClassifier object)\n",
    " *         base_estimator_ : estimator - The base estimator from which the ensemble is grown.\n",
    " *         estimators_ : list of classifiers - The collection of fitted sub-estimators as defined in estimators that are not ‘drop’.\n",
    " *         classes_ : ndarray of shape (n_classes,) - The classes labels.\n",
    " *         n_classes_ : int - The number of classes.\n",
    " *         estimator_weights_ : ndarray of floats - Weights for each estimator in the boosted ensemble.\n",
    " *         estimator_errors_ : ndarray of floats - Classification error for each estimator in the boosted ensemble.\n",
    " *         feature_importances_ : ndarray of shape (n_features,) - The impurity-based feature importances.\n",
    " \n",
    "  #### Methods: (on the VotingClassifier object)\n",
    " * decision_function(self, X) : Compute the decision function of X.\n",
    " * fit(self, X, y[, sample_weight]) : Build a boosted classifier from the training set (X, y).\n",
    " * get_params(self[, deep]) : Get parameters for this estimator.\n",
    " * predict(self, X) :  Predict classes for X.\n",
    " * predict_log_proba(self, X) : Predict class log-probabilities for X.\n",
    " * predict_proba(self, X) : Predict class probabilities for X.\n",
    " * score(self, X, y[, sample_weight]) : Return the mean accuracy on the given test data and labels.\n",
    " * set_params(self, \\*\\*params) : Set the parameters of this estimator.\n",
    " * staged_decision_function(self, X) : Compute decision function of X for each boosting iteration.\n",
    " * staged_predict(self, X) : Return staged predictions for X.\n",
    " * staged_predict_proba(self, X) : Predict class probabilities for X.\n",
    " * staged_score(self, X, y[, sample_weight]) : Return staged scores for X, y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ex. 3.b.1: AdaBoostClassifier\n",
    "\n",
    "# Set up the estimator range\n",
    "estimator = list(range(1,101))\n",
    "# To find best number of estimator, initialize best_estimator and max_score, and best_model\n",
    "best_estimator = -1\n",
    "max_score = -1\n",
    "best_model = None\n",
    "# To plot the trend of score along the estomator, initialize the list of score\n",
    "score_list = []\n",
    "for n_estimators in estimator :\n",
    "    ##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "    # Create AdaBoostClassifier with n_estimators.\n",
    "    # Hint: The number of estimators are configured by 'n_estimators' parameter.\n",
    "    # Hint: We use 0 as seed which is given at each base_estimator at each boosting iteration.\n",
    "    AdaBoost_clf = None\n",
    "    k_folds = KFold(n_splits=20)\n",
    "    results = cross_val_score(AdaBoost_clf, X_train, y_train, cv=k_folds)\n",
    "    score = results.mean()\n",
    "    score_list.append(score)\n",
    "    if score > max_score :\n",
    "        max_score = score\n",
    "        best_estimator = n_estimators\n",
    "        best_model = AdaBoost_clf\n",
    "plt.plot(estimator, score_list)\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.ylabel('Prediction accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best estimator number and the maximum score\n",
    "print(\"Best Estimator number : {} / Best prediction accuracy : {}\".format(best_estimator, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. 3.b.2: plot feature importances\n",
    "\n",
    "# Create AdaBoostClassifier\n",
    "# Hint: the number of estimators giving the best performance is best_estimator.\n",
    "AdaBoost_clf = None\n",
    "AdaBoost_clf.fit(X_train, y_train)\n",
    "fig, ax = plt.subplots()\n",
    "x=list(df.columns[:-1])\n",
    "\n",
    "##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "# Plot AdaBoost classifier's feature importances\n",
    "ax.plot_date(x, None, 'bo')\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensemble Learning\n",
    "----------------------\n",
    "## c. BaggingClassifier (Bagging)\n",
    "----------------------\n",
    "* A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregates their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "### BaggingClassifier(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
    " #### Arguments: only \"n_estimators\" matters for now\n",
    " 1.         base_estimator : object, default=None - The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes. If None, then the base estimator is DecisionTreeClassifier(max_depth=1).\n",
    "  2.        n_estimators : int, default= 10 - The number of base estimators in the ensemble.\n",
    "  3.        max_features : int or float, default=1.0 - The number of features to draw from X to train each base estimator (without replacement by default)\n",
    "  4.        bootstrap : bool, default=True -  Whether samples are drawn with replacement. If False, sampling without replacement is performed.\n",
    "  5.        bootstrap_features : bool, default=False - Whether features are drawn with replacement.\n",
    "  6.        oob_score : bool, default=False - Whether to use out-of-bag samples to estimate the generalization error.\n",
    "  7.         warm_start : bool, default=False - When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble.\n",
    "  8.         n_jobs : int, default=None -  The number of jobs to run in parallel for both fit and predict. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.\n",
    "  9.         random_state : int or RandomState, default=None - Controls the random resampling of the original dataset (sample wise and feature wise). If the base estimator accepts a random_state attribute, a different seed is generated for each instance in the ensemble. Pass an int for reproducible output across multiple function calls.\n",
    "  10.        verbose : bool, default=False - Controls the verbosity when fitting and predicting.\n",
    "  \n",
    " #### Artributes: (of the VotingClassifier object)\n",
    " *         base_estimator_ : estimator - The base estimator from which the ensemble is grown.\n",
    " *         estimators_ : list of classifiers - The collection of fitted sub-estimators as defined in estimators that are not ‘drop’.\n",
    " *         estimators_samples_ : list of arrays - The subset of drawn samples for each base estimator.\n",
    " *         estimators_features_ : list of arrays - The subset of drawn features for each base estimator.\n",
    " *         classes_ : ndarray of shape (n_classes,) - The classes labels.\n",
    " *         n_classes_ : int - The number of classes.\n",
    " *         oob_score_ : float - Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when oob_score is True.\n",
    " *         oob_decision_function_ : ndarray of shape (n_samples, n_classes) - Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, oob_decision_function_ might contain NaN. This attribute exists only when oob_score is True.\n",
    "  #### Methods: (on the VotingClassifier object)\n",
    " * decision_function(self, X) : Compute the decision function of X.\n",
    " * fit(self, X, y[, sample_weight]) : Build a boosted classifier from the training set (X, y).\n",
    " * get_params(self[, deep]) : Get parameters for this estimator.\n",
    " * predict(self, X) :  Predict classes for X.\n",
    " * predict_log_proba(self, X) : Predict class log-probabilities for X.\n",
    " * predict_proba(self, X) : Predict class probabilities for X.\n",
    " * score(self, X, y[, sample_weight]) : Return the mean accuracy on the given test data and labels.\n",
    " * set_params(self, \\*\\*params) : Set the parameters of this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ex. 3.c.1: BaggingClassifier\n",
    "\n",
    "# Set up the estimator range\n",
    "estimator = list(range(1,101))\n",
    "# To find best number of estimator, initialize best_estimator and max_score, and best_model\n",
    "best_estimator = -1\n",
    "max_score = -1\n",
    "best_model = None\n",
    "# To plot the trend of score along the estomator, initialize the list of score\n",
    "score_list = []\n",
    "for n_estimators in estimator :\n",
    "    ##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "    # Create BaggingClassifier instance.\n",
    "    # Hint: The number of estimators are configured by 'n_estimators' parameter.\n",
    "    Bagging_clf = None\n",
    "    k_folds = KFold(n_splits=20)\n",
    "    results = cross_val_score(Bagging_clf, X_train, y_train, cv=k_folds)\n",
    "    score = results.mean()\n",
    "    score_list.append(score)\n",
    "    if score > max_score :\n",
    "        max_score = score\n",
    "        best_estimator = n_estimators\n",
    "        best_model = Bagging_clf\n",
    "plt.plot(estimator, score_list, 'r')\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.ylabel('Prediction accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best estimator number and the maximum score\n",
    "print(\"Best Estimator number : {} / Best prediction accuracy : {}\".format(best_estimator, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ex. 3.c.2: plot feature importances\n",
    "\n",
    "best_model.fit(X_train, y_train)\n",
    "#\n",
    "##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "# Hint: Find the feature importance of BaggingClassifier by averaging feature importance of base estimators (Use attribute 'estimators_')\n",
    "feature_importances = np.mean([\n",
    "    None\n",
    "], axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "x=list(df.columns[:-1])\n",
    "ax.plot_date(x, feature_importances, 'ro')\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ensemble Learning\n",
    "----------------------\n",
    "## d. RandomForestClassifier\n",
    "----------------------\n",
    "* A random forest classifier. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n",
    "### RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    " \n",
    " #### Arguments: only \"n_estimators\" matters for now\n",
    "  1.        n_estimators : int, default= 10 - The number of base estimators in the ensemble.\n",
    "  2.        criterion : {“gini”, “entropy”}, default=”gini”- The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.\n",
    "  3.        max_depth : int, default=None - The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "  4.        min_samples_split : int or float, default=2 - The minimum number of samples required to split an internal node: If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a fraction and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "  5.        min_samples_leaf : int or float, default=1 - The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. If int, then consider min_samples_leaf as the minimum number. If float, then min_samples_leaf is a fraction and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.\n",
    "  6.        min_weight_fraction_leaf : float, default=0.0 - The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n",
    "  7.        max_features : {“auto”, “sqrt”, “log2”}, int or float, default=”auto”- The number of features to consider when looking for the best split:\n",
    "    * If int, then consider max_features features at each split.\n",
    "    * If float, then max_features is a fraction and int(max_features * n_features) features are considered at each split.\n",
    "    * If “auto”, then max_features=sqrt(n_features).\n",
    "    * If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "    * If “log2”, then max_features=log2(n_features).\n",
    "    * If None, then max_features=n_features.\n",
    "  \n",
    " #### Artributes: (of the VotingClassifier object)\n",
    " *         base_estimator_ : The child estimator template used to create the collection of fitted sub-estimators.\n",
    " *         estimators_ : list of DecisionTreeClassifier - The collection of fitted sub-estimators.\n",
    " *         classes_ : ndarray of shape (n_classes,) - The classes labels.\n",
    " *         n_classes_ : int - The number of classes.\n",
    " *         n_features_ : int - The number of features when fit is performed.\n",
    " *         n_outputs_int : The number of outputs when fit is performed.\n",
    " *         feature_importances_ : ndarray of shape (n_features,) - The impurity-based feature importances.\n",
    " * oob_score_ : float - Score of the training dataset obtained using an out-of-bag estimate. This attribute exists only when oob_score is True.\n",
    " * oob_decision_function_ : ndarray of shape (n_samples, n_classes) - Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, oob_decision_function_ might contain NaN. This attribute exists only when oob_score is True.\n",
    "\n",
    " #### Methods: (on the VotingClassifier object)\n",
    " * apply(self, X) : Apply trees in the forest to X, return leaf indices.\n",
    " * decision_path(self, X) : Return the decision path in the forest.\n",
    " * fit(self, X, y[, sample_weight]) : Build a forest of trees from the training set (X, y).\n",
    " * get_params(self[, deep]) : Get parameters for this estimator.\n",
    " * predict(self, X) :  Predict classes for X.\n",
    " * predict_log_proba(self, X) : Predict class log-probabilities for X.\n",
    " * predict_proba(self, X) : Predict class probabilities for X.\n",
    " * score(self, X, y[, sample_weight]) : Return the mean accuracy on the given test data and labels.\n",
    " * set_params(self, \\*\\*params) : Set the parameters of this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. 3.d.1: RandomForestClassifier\n",
    "\n",
    "# Set up the estimator range\n",
    "estimator = list(range(1,101))\n",
    "# To find best number of estimator, initialize best_estimator and max_score, and best_model\n",
    "best_estimator = -1\n",
    "max_score = -1\n",
    "best_model = None\n",
    "# To plot the trend of score along the estomator, initialize the list of score\n",
    "score_list = []\n",
    "for n_estimators in estimator :\n",
    "    ##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "    # Create RandomForestClassifier instance.\n",
    "    # Hint: The number of estimators are configured by 'n_estimators' parameter.\n",
    "    # Hint: Set base estimators(Decision Tree) to use 'entropy' as criterion.\n",
    "    RandomF_clf = None\n",
    "    k_folds = KFold(n_splits=20)\n",
    "    results = cross_val_score(RandomF_clf, X_train, y_train, cv=k_folds)\n",
    "    score = results.mean()\n",
    "    score_list.append(score)\n",
    "    if score > max_score :\n",
    "        max_score = score\n",
    "        best_estimator = n_estimators\n",
    "        best_model = RandomF_clf\n",
    "plt.plot(estimator, score_list, 'g')\n",
    "plt.xlabel('Number of estimators')\n",
    "plt.ylabel('Prediction accuracy(%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the best estimator number and the maximum score\n",
    "print(\"Best Estimator number : {} / Best prediction accuracy : {}\".format(best_estimator, max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. 3.d.2: plot feature importance\n",
    "\n",
    "##### YOUR CODE HERE (Fill in the \"None\") #####\n",
    "# Plot RandomForestClassifier's feature importances\n",
    "best_model.fit(X_train, y_train)\n",
    "feature_importances = None\n",
    "fig, ax = plt.subplots()\n",
    "x=list(df.columns[:-1])\n",
    "ax.plot_date(x, feature_importances, 'ro')\n",
    "fig.autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "1. https://scikit-learn.org/stable/modules/ensemble.html\n",
    "2. https://towardsdatascience.com/ensemble-learning-using-scikit-learn-85c4531ff86a  \n",
    "3. https://stackabuse.com/ensemble-voting-classification-in-python-with-scikit-learn/ \n",
    "4. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n",
    "5. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "6. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "7. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
